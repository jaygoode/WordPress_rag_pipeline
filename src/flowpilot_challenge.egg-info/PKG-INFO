Metadata-Version: 2.4
Name: flowpilot-challenge
Version: 0.1.0
Summary: Challenge scaffold
Author: Zoner
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: typer[all]>=0.12
Requires-Dist: pydantic>=2.7
Requires-Dist: pydantic-settings>=2.2
Requires-Dist: python-dotenv>=1.0
Requires-Dist: rich>=13.7
Requires-Dist: structlog>=24.1
Requires-Dist: orjson>=3.10
Requires-Dist: datasets>=2.18
Requires-Dist: transformers>=4.40
Requires-Dist: sentence-transformers>=2.6
Requires-Dist: accelerate>=0.28
Requires-Dist: faiss-cpu>=1.7.4
Requires-Dist: langchain>=0.2
Requires-Dist: langchain-community>=0.2
Requires-Dist: langchainhub>=0.1
Requires-Dist: chromadb>=0.5
Requires-Dist: fastapi>=0.111
Requires-Dist: uvicorn[standard]>=0.29
Requires-Dist: psycopg[binary]>=3.1
Provides-Extra: dev
Requires-Dist: pytest>=8.2; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23; extra == "dev"
Requires-Dist: pytest-cov>=5.0; extra == "dev"
Requires-Dist: mypy>=1.10; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Requires-Dist: types-requests; extra == "dev"

# RAG Retrieval Pipeline Challenge

You'll receive a messy WordPress QA dataset ( https://huggingface.co/datasets/mteb/cqadupstack-wordpress ) and transform it into a functional retrieval system. We provide the scaffolding (CLI commands, settings, optional Docker infrastructure). The implementation is entirely yours.

## Core Requirements

The task centers on two components. First, build an ingestion pipeline that cleans the raw data, chunks it appropriately, and persists it to the provided Postgres + pgvector instance (see `docker-compose.yml`). If you prefer an alternative to pgvector, document your reasoning. Second, design and implement the retrieval system itself: embedding strategy, indexing approach, and query handling. We intentionally provide no defaults here.

The core work should take 2-3 hours. If you finish early or want to demonstrate additional capabilities, consider adding a reranker or implementing a multi-turn conversational agent. These are strictly optional.

## Repository Structure
```
.
├── Makefile                  
├── docker-compose.yml        
├── pyproject.toml            
├── src/agentic_rag/          
└── scripts/download_dataset.py
```

Reorganize as needed, but preserve the CLI commands for reproducibility.

## Setup and Execution
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
make compose-up        
make data              
make ingest            
make agent             
```

Stop the pgvector service with `make compose-down`. If the dataset requires authentication, export `HF_TOKEN` before downloading.

## Configuration

Specify your implementations via environment variables or `.env`:
```bash
export AGENTIC_RAG_INGESTION_CLASS="my_pkg.ingestion.Pipeline"
export AGENTIC_RAG_AGENT_CONTROLLER_CLASS="my_pkg.agent.Controller"
```

## Submission

Provide a repository link with clear execution instructions. Include a brief architectural overview that addresses your design decisions: embedding model selection, chunking strategy, pgvector schema and indexing approach, known limitations, and potential improvements. If you deviated from pgvector, explain the alternative and its trade-offs.

Additional artifacts (tests, documentation, tooling) are at your discretion.



